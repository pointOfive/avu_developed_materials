---
title: "ISLR Problem 3.15"
author: "**SOLUTIONS**"
#author: "**Your Name Here**"
#date: "May 16, 2020"
output: html_document # uncomment and comment the following line to switch
#output: html_notebook  # notebooks may be a better option for homework
---

<!--- Below are global settings for knitr. You can override any of them by adding the changes to individual chunks --->


```{r global_options, include=FALSE}
knitr::opts_chunk$set(error=TRUE,        # Keep compiling upon error
                      collapse=FALSE,    # collapse by default
                      echo=TRUE,         # echo code by default
                      comment = "#>",    # change comment character
                      fig.width = 5.5,     # set figure width
                      fig.align = "center",# set figure position
                      out.width = "49%", # set width of displayed images
                      warning=FALSE,     # do not show R warnings
                      message=FALSE)     # do not show R messages
```

<!--- Change font size for headers --->
<style>
h1.title {
  font-size: 28px;
}
h1 {
  font-size: 22px;
}
h2 {
  font-size: 18px;
}
h3 { 
  font-size: 14px;
}

</style>



**SYS 6018 | Summer 2020 | University of Virginia **

Solutions to the homework problems in [Introduction to Statistical Learning (ISLR)](https://faculty.marshall.usc.edu/gareth-james/ISL/).

*******************************************

Solutions will be shown using Base R (following the textbook) and the [Tidyverse](https://www.tidyverse.org/learn/) dialect of R. Either (or a mix of both) is acceptable, but I encourage tidyverse for those who can take the extra time to deviate from the textbook. It is well worth the investment.

To use tidyverse solutions, we will need to load the following packages:
```{r, include=FALSE}
library(tidyverse)  # ggplot2, dplyr, etc.
library(broom)      # to get tidy model output
library(knitr)      # for the kable() function for tables
library(kableExtra) # to make fancier html tables

# Note: if you don't have a package installed on your machine you will need to install them. In RStudio, to to Tools->Install Packages... or just run `install.packages()`. For example: install.packages("tidyverse")
```


Use the `Boston` data from the `MASS` package. Note: if you load the `MASS` package after `tidyverse` (or `dplyr` specifically) then the `dplyr::select()` function will be masked by the `MASS::select()` function. Since we don't need anything else in `MASS` except the data, we will just load the data (and convert to a tibble)

```{r, include=FALSE}
Boston = MASS::Boston %>% as_tibble()
```


## (a). Predict Per Capita Crime Rate (`crim`) using univariate predictors {.tabset}

For each predictor, it a simple linear regression model to predict
the response. Describe your results. In which of the models is
there a statistically significant association between the predictor
and the response? Create some plots to back up your assertions.

### Tidyverse

The first helpful thing to do is make the data *long* so we can group by the predictor variables (nesting is another good option). 
```{r}
Boston_long = Boston %>% 
  pivot_longer(cols=-crim, names_to = 'predictor', values_to='x')

Boston_long
```

This makes plotting with facets easy:
```{r}
Boston_long %>% 
  ggplot(aes(x, crim)) + 
  geom_smooth(method='lm') + 
  geom_point() + 
  facet_wrap(~predictor, scales="free_x") + 
  labs(x="predictor value", y="Per Capita Crime Rate")
```
This should make you concerned. Notice that several of the predictors have many duplicate $x$ values, but very different $y$ values. I would want to check into this and feel comfortable this is correct. This may prompt me to consider two models - one for the similar suburbs and another for the others. 

We can also use the long format to get the regression model coefficients and statistics. Here I will group by the predictors and fit a simple linear model followed by a `broom::tidy()` to ensure a data frame is returned. The `dplyr::do()` function allows any function to be applied to the grouped data. 
```{r}
Boston_long %>% 
  group_by(predictor) %>% 
  do(lm(crim~x, data=.) %>% broom::tidy()) %>% # get the coefficients
  filter(term == 'x') %>%            # we don't care about the intercept
  arrange(p.value) %>%               # arrange by ascending p-values
  select(-term)                      # drop the term column
```
So everything but `chas` (Charles River dummy variable) is statistically significant. 



### Base R

I'll run each model separately using a loop and saving the results in a list:
```{r}
predictors = names(Boston) %>% setdiff("crim") # all predictors

models = list()   # initialize a list object
for(x in predictors) {
  fmla = paste('crim ~', x)           # make a formula 
  models[[x]] = lm(fmla, data=Boston) # fit linear regression model
}
```

Now we can extract what we want from each model; here it is the p values for all coefficients
```{r}
lapply(models, function(x) broom::tidy(x))
```

Everything statistically significant except `chas` (Charles River dummy). But why are we running each predictor independently? Let's generate some plots and see if anything strange is going on (as the textbook hints). 

```{r, out.width="85%"}
par(mfrow=c(4,4), mar=c(4,4,1,.5))  # set up a 4x4 plotting region
for(i in 1:length(models)) {
  x = names(models)[i]  # predictor variable
  plot(Boston[[x]], Boston$crim,  # scatterplot
       las=1, pch=19,             # rotate y labels, solid points
       xlab=x, ylab="crime rate")
  abline(models[[i]], col="blue") # add linear fit
}
```
This should make you concerned. Notice that several of the predictors have many duplicate $x$ values, but very different $y$ values. I would want to check into this and feel comfortable this is correct. This may prompt me to consider two models - one for the similar suburbs and another for the others. 


## (b). Predict Per Capita Crime Rate (`crim`) using all predictors {.tabset}

```{r}
fit.full = lm(crim~., data=Boston)  # fit full model
beta = broom::tidy(fit.full)        # extract coefficients
signif = which(beta$p.value <= .05) # find the ones with p.value<=.05
 
#- make nice table output using the kableExtra package
beta %>% 
  knitr::kable(digits=3) %>% 
  kableExtra::kable_styling(full_width = FALSE) %>% 
  row_spec(signif, bold = TRUE, background = "#D3D3D3")
```
Now only 5 predictors are statistically significant (at the 5% level). 

## (c). Compare univariate to multivariate {.tabset}

### Tidyverse

```{r}
beta.univariate = 
  Boston_long %>% 
  group_by(predictor) %>% 
  do(lm(crim~x, data=.) %>% broom::tidy()) %>% # get the coefficients
  filter(term == 'x') %>%            # we don't care about the intercept
  select(predictor, estimate)    

beta.full = lm(crim~., data=Boston) %>% 
  broom::tidy() %>% 
  select(predictor=term, estimate)

left_join(beta.univariate, beta.full, by="predictor") %>% 
  ggplot(aes(estimate.x, estimate.y)) + 
  geom_point() + 
  geom_text(aes(label=predictor), position=position_jitter(width=1, height=1)) +
  labs(x="univariate", y="multivariate")
```
The `nox` predictor shows the largest absolute difference. 


### Base R

```{r}
#-- Full Model
fit.full = lm(crim~., data=Boston)  # fit full model
beta = broom::tidy(fit.full)


#-- Univariate Models
predictors = names(Boston) %>% setdiff("crim") # all predictors

beta.uni = data.frame(predictor=predictors, estimate=NA)   # initialize data.frame
for(x in predictors) {
  fmla = paste('crim ~', x)           # make a formula 
  fit = lm(fmla, data=Boston)         # fit linear regression model
  beta.uni$estimate[beta.uni$predictor==x] = coef(fit)[2]
}

plot(beta.uni$estimate, beta$estimate[-1], 
     las=1, pch=19, 
     xlab="univariate", ylab="multivariate")
```
The `nox` predictor shows the largest absolute difference. 



## (d). Check for non-linear terms with a cubic model (for each predictor). 

Technical note: a polynomial expansion will only work for predictors that have more than $d$ unique values. The binary `chas` predictor only has two levels, so we will exclude it. 

### Tidyverse

```{r}
Boston_long %>% 
  filter(predictor != 'chas') %>% # remove binary predictor
  group_by(predictor) %>% 
  #- fit cubic models
  do(lm(crim~poly(x, 3), data=.) %>% broom::tidy()) %>% # get the coefficients
  #- transform for human output
  filter(term != '(Intercept)') %>%  # we don't care about the intercept
  select(predictor, term, p.value) %>% # keep columns we care about
  pivot_wider(names_from=term, values_from=p.value) %>%  # convert to long format
  #- make nicer table using the kableExtra package
  knitr::kable(digits=3) %>% 
  kableExtra::kable_styling("striped", full_width = FALSE)
```
All predictors except `black` have a significant *quadratic* term (at the 5% level). Several have significant *cubic* terms. 

Don't read much into this or consider this univariate analysis standard practice. I see this exercise more useful in thinking about how to work with multiple models and manipulate and summarize their output. 

We are concerned with predictive ability (predictive inference) in this course, not model inference. We will use any predictors (and derived predictors/features) that provide the best predictions (usually evaluated with hold-out data). So I wouldn't really do these univariate analyses except for data exploration (e.g., finding outliers or unexpected values like the duplicates identified in part a), but this is usually easier with plots than models. 


### Base R

I'll run each model separately using a loop and saving the results in a list:
```{r}
predictors = names(Boston) %>% setdiff(c("crim","chas")) # almost all predictors

models = list()   # initialize a list object
for(x in predictors) {
  fmla = paste('crim ~ poly(', x, ", 3)")           # make a formula 
  models[[x]] = lm(fmla, data=Boston) # fit linear regression model
}
```

Now we can extract what we want from each model; here it is the p values for all coefficients
```{r}
lapply(models, function(x) broom::tidy(x))
```

All predictors except `black` have a significant *quadratic* term (at the 5% level). Several have significant *cubic* terms. 

Don't read much into this or consider this univariate analysis standard practice. I see this exercise more useful in thinking about how to work with multiple models and manipulate and summarize their output. 

We are concerned with predictive ability (predictive inference) in this course, not model inference. We will use any predictors (and derived predictors/features) that provide the best predictions (usually evaluated with hold-out data). So I wouldn't really do these univariate analyses except for data exploration (e.g., finding outliers or unexpected values like the duplicates identified in part a), but this is usually easier with plots than models. 
